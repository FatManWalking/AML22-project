# This file is used as a baseline
# These are all the possible arguments to configure the trainer
# Please changes the ones you want to change
# and delete all options you do not want to call explicity

# The name of the task you are training
task_type: "graph classification"
# GPU or CPU
device: "cuda"
# an identifyable name (suffix)
experiment_name: "hpt-75"
experiment_name_suffix: ""
# Dataset (provided by torch.gym)
dataset: "FRANKENSTEIN" # DD/PROTEINS/NCI1/NCI109/Mutagenicity/ENZYMES
# Random Seed to generate reproducable random things
random_seed: 42

# The architecture of the model you're using
architecture: "hgp"
# Strucure Learning
structure_learning: True
# Resume training
resume_from_checkpoint: False
# Number of Workers
num_workers: 4
# load the best model after training
load_best: True
# Checkpoint save location (will create a subfolder with the experiment name --> /model/test/..)
output_path: "model"
# is a higher value better in the metric you use?
greater_better: True
# metric you want to use
metric_used: "matthews_correlation"
# Split Ratio
split_ratio: 0.7
# Ratio between Eval and Test data (0.5 = 50/50). Higher means more Eval
test_ratio: 0.5
#Hyperparameters
# Batch Size
batch_size: 400
# Learning Rate
lr: 1e-03
weight_decay: 1e-03
# Warm up for the learning rate
warm_up: 1/5
# Hidden Size
nhid: 128
# LR Schedule to adept over time
lr_scheduler_type: "linear"
# Epochs
epochs: 40
# Early stopping
patience: 100
# Custom Trainer
custom_trainer: "TrainerDiceLoss"
# Pooling Ratio
pooling_ratio: 0.3
# Dropout Ratio
dropout_ratio: 1e-03
# Lamb - The tradeoff parameter
lamb: 1
# Neighbor sampling
sample_neighbor: True
# Sparse Attention
sparse_attention: True
# Structure Learning
structure_learning: True
# Pruning
pruning: True


# Should there be a tensorboard log?
logging: True
log_path: "log"
logging_strategy: "steps"
log_steps: 1
evaluation_strategy: "steps"
evaluation_steps: 1
save_strategy: "steps"
save_steps: 100
save_limits: 5
